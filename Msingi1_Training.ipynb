{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Msingi1_Training.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Msingi-AI/msingi1/blob/main/Msingi1_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "# Msingi1: Swahili Language Model Training\n",
    "\n",
    "This notebook trains a Mixture of Experts (MoE) language model for Swahili. Follow these steps:\n",
    "\n",
    "1. Mount Google Drive\n",
    "2. Clone repository and install dependencies\n",
    "3. Upload dataset and tokenizer\n",
    "4. Train model with checkpointing\n",
    "5. Generate text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "!mkdir -p /content/drive/MyDrive/msingi1/{data,tokenizer,checkpoints,logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone repository and install dependencies\n",
    "!git clone https://github.com/Msingi-AI/msingi1.git\n",
    "%cd msingi1\n",
    "\n",
    "!pip install -q torch==2.0.1 fastmoe==0.3.2 wandb tokenizers datasets tqdm\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# Upload dataset and tokenizer files\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def upload_to_drive(local_path, drive_path):\n",
    "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
    "    shutil.copy2(local_path, drive_path)\n",
    "    print(f\"Uploaded {local_path} to {drive_path}\")\n",
    "\n",
    "# Upload dataset\n",
    "print(\"Upload your Swahili dataset file...\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    drive_path = f\"/content/drive/MyDrive/msingi1/data/{filename}\"\n",
    "    upload_to_drive(filename, drive_path)\n",
    "\n",
    "# Upload tokenizer files\n",
    "print(\"\\nUpload tokenizer files (vocab.json and merges.txt)...\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    drive_path = f\"/content/drive/MyDrive/msingi1/tokenizer/{filename}\"\n",
    "    upload_to_drive(filename, drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from src.model import Msingi1, MsingiConfig\n",
    "from src.data_processor import SwahiliDataset\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"/content/drive/MyDrive/msingi1/tokenizer/vocab.json\",\n",
    "    \"/content/drive/MyDrive/msingi1/tokenizer/merges.txt\"\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "config = MsingiConfig(\n",
    "    vocab_size=32000,\n",
    "    max_position_embeddings=1024,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    num_experts=8,\n",
    "    expert_capacity=32,\n",
    "    moe_layers=[2, 4]\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = {\n",
    "    'batch_size': 16,  # Reduced for GPU memory\n",
    "    'accumulation_steps': 4,  # Effective batch size = 64\n",
    "    'learning_rate': 3e-4,\n",
    "    'warmup_steps': 1000,\n",
    "    'max_steps': 20000,\n",
    "    'save_steps': 1000,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"msingi1\",\n",
    "    config={\n",
    "        **vars(config),\n",
    "        **training_args\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "dataset = SwahiliDataset(\n",
    "    data_path=\"/content/drive/MyDrive/msingi1/data/train.txt\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=training_args['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = Msingi1(config).cuda()\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=training_args['learning_rate'],\n",
    "    weight_decay=training_args['weight_decay']\n",
    ")\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint_dir = \"/content/drive/MyDrive/msingi1/checkpoints\"\n",
    "latest_checkpoint = None\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = sorted(os.listdir(checkpoint_dir))\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "        print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "        checkpoint = torch.load(latest_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_step = checkpoint['step']\n",
    "    else:\n",
    "        start_step = 0\n",
    "else:\n",
    "    start_step = 0\n",
    "\n",
    "# Training loop\n",
    "step = start_step\n",
    "optimizer.zero_grad()\n",
    "\n",
    "while step < training_args['max_steps']:\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Step {step}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, config.vocab_size),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / training_args['accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights if we've accumulated enough gradients\n",
    "        if (step + 1) % training_args['accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                training_args['max_grad_norm']\n",
    "            )\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'loss': loss.item() * training_args['accumulation_steps'],\n",
    "            'step': step\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if step > 0 and step % training_args['save_steps'] == 0:\n",
    "            checkpoint_path = f'/content/drive/MyDrive/msingi1/checkpoints/step_{step}.pt'\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f'\\nSaved checkpoint to {checkpoint_path}')\n",
    "        \n",
    "        # Update progress\n",
    "        progress_bar.set_postfix({'loss': loss.item() * training_args['accumulation_steps']})\n",
    "        step += 1\n",
    "        \n",
    "        if step >= training_args['max_steps']:\n",
    "            break\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'step': step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config\n",
    "}, '/content/drive/MyDrive/msingi1/checkpoints/final_model.pt')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate"
   },
   "outputs": [],
   "source": [
    "# Generate text samples\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, max_length=100, temperature=0.7, top_k=50):\n",
    "    with torch.no_grad():\n",
    "        # Tokenize prompt\n",
    "        input_ids = torch.tensor(tokenizer.encode(prompt).ids).unsqueeze(0).cuda()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = F.softmax(top_k_logits, dim=-1)\n",
    "            next_token = top_k_indices[0, torch.multinomial(probs[0], 1)]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Stop if we predict the end token\n",
    "            if next_token.item() == tokenizer.token_to_id(\"</s>\"):\n",
    "                break\n",
    "        \n",
    "        return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"Habari ya leo\",\n",
    "    \"Naomba\",\n",
    "    \"Karibu\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")"
   ]
  }
 ]
}
