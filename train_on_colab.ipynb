{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Msingi1: A Swahili Language Model\n",
    "\n",
    "This notebook trains the Msingi1 language model on Google Colab using TPU/GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's set up our environment and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/YOUR_USERNAME/msingi1.git\n",
    "%cd msingi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html  # For TPU support\n",
    "!pip install wandb  # For experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Available Hardware\n",
    "\n",
    "Let's verify what hardware accelerator we have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "elif os.environ.get('COLAB_TPU_ADDR'):\n",
    "    print(\"TPU available\")\n",
    "    device = 'tpu'\n",
    "else:\n",
    "    print(\"No GPU/TPU found, using CPU\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive\n",
    "\n",
    "We'll mount Google Drive to save our checkpoints and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p checkpoints\n",
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset\n",
    "\n",
    "Copy your dataset to the appropriate location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Copy dataset from Drive if needed\n",
    "!cp -r \"/content/drive/MyDrive/path/to/swahili/data\" data/\n",
    "\n",
    "# Or download dataset directly\n",
    "# !wget -O data/swahili_data.zip URL_TO_YOUR_DATASET\n",
    "# !unzip data/swahili_data.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Tokenizer\n",
    "\n",
    "First, let's train our custom ByteLevelBPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.train_tokenizer import train_tokenizer\n",
    "\n",
    "# Train tokenizer\n",
    "train_tokenizer(\n",
    "    data_path=\"data/Swahili data/Swahili data/train.txt\",\n",
    "    save_dir=\"tokenizer\",\n",
    "    vocab_size=50000,\n",
    "    min_frequency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Training\n",
    "\n",
    "Now let's set up our model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from src.model import MsingiConfig\n",
    "from src.train import train\n",
    "from src.data_processor import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "train_texts = load_dataset('data/Swahili data/Swahili data/train.txt')\n",
    "print(f\"Loaded {len(train_texts)} text samples\")\n",
    "\n",
    "# Initialize config\n",
    "config = MsingiConfig(\n",
    "    vocab_size=50000,\n",
    "    max_position_embeddings=2048,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    gradient_checkpointing=True  # Enable for memory efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training\n",
    "\n",
    "Finally, let's start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start training\n",
    "train(\n",
    "    config=config,\n",
    "    train_texts=train_texts,\n",
    "    val_texts=None,  # We'll use a portion of train data for validation\n",
    "    num_epochs=100,\n",
    "    batch_size=4,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=1000,\n",
    "    grad_acc_steps=16,\n",
    "    save_steps=1000,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model to Drive\n",
    "\n",
    "After training, let's save our model to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create model directory in Drive\n",
    "!mkdir -p \"/content/drive/MyDrive/msingi1/models\"\n",
    "\n",
    "# Copy checkpoints to Drive\n",
    "!cp -r checkpoints/* \"/content/drive/MyDrive/msingi1/models/\"\n",
    "!cp -r tokenizer \"/content/drive/MyDrive/msingi1/\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Train Msingi1 Language Model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
