{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# Msingi1: Swahili Language Model with Mixture of Experts\n",
    "\n",
    "This notebook trains the Msingi1 model using Mixture of Experts on Google Colab's GPU.\n",
    "\n",
    "1. Click Runtime -> Change runtime type\n",
    "2. Select GPU as Hardware accelerator\n",
    "3. Click Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "# Mount Google Drive for checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p /content/drive/MyDrive/msingi1/{checkpoints,logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Msingi-AI/msingi1.git\n",
    "%cd msingi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch==2.0.1 fastmoe==0.3.2 wandb tokenizers datasets tqdm\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.model import Msingi1, MsingiConfig\n",
    "from src.data_processor import SwahiliDataset\n",
    "\n",
    "# Enable tensor cores for faster training\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "source": [
    "# Model and training configuration\n",
    "config = MsingiConfig(\n",
    "    vocab_size=32000,\n",
    "    max_position_embeddings=1024,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    num_experts=8,\n",
    "    expert_capacity=32,\n",
    "    moe_layers=[2, 4]\n",
    ")\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 3e-4,\n",
    "    'warmup_steps': 1000,\n",
    "    'max_steps': 20000,\n",
    "    'save_steps': 1000,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "source": [
    "# Initialize wandb for experiment tracking\n",
    "wandb.init(\n",
    "    project=\"msingi1\",\n",
    "    config={\n",
    "        **vars(config),\n",
    "        **training_args\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "dataset = SwahiliDataset('data/Swahili data/train.txt')\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=training_args['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model and move to GPU\n",
    "model = Msingi1(config).cuda()\n",
    "model.train()\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=training_args['learning_rate'],\n",
    "    weight_decay=training_args['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(step):\n",
    "    if step < training_args['warmup_steps']:\n",
    "        return step / training_args['warmup_steps']\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "source": [
    "# Training loop\n",
    "step = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "while step < training_args['max_steps']:\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Step {step}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, config.vocab_size),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / training_args['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights if we've accumulated enough gradients\n",
    "        if (step + 1) % training_args['gradient_accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                training_args['max_grad_norm']\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'loss': loss.item() * training_args['gradient_accumulation_steps'],\n",
    "            'learning_rate': scheduler.get_last_lr()[0],\n",
    "            'step': step\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if step > 0 and step % training_args['save_steps'] == 0:\n",
    "            checkpoint_path = f'/content/drive/MyDrive/msingi1/checkpoints/step_{step}.pt'\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f'\\nSaved checkpoint to {checkpoint_path}')\n",
    "        \n",
    "        # Update progress\n",
    "        progress_bar.set_postfix({'loss': loss.item() * training_args['gradient_accumulation_steps']})\n",
    "        step += 1\n",
    "        \n",
    "        if step >= training_args['max_steps']:\n",
    "            break\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'step': step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config\n",
    "}, '/content/drive/MyDrive/msingi1/checkpoints/final_model.pt')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate"
   },
   "source": [
    "# Generate some text\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prompt = \"Habari ya leo\"\n",
    "    generated = model.generate(\n",
    "        prompt,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        top_k=50\n",
    "    )\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
