{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Msingi1: Training on TPU with JAX/Flax\n",
    "\n",
    "This notebook trains the Msingi1 Swahili language model using Google Colab's TPU for maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check TPU availability and install dependencies\n",
    "!pip install -q jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install -q flax optax transformers datasets wandb\n",
    "\n",
    "import jax\n",
    "print('Number of TPU devices:', jax.device_count())\n",
    "print('JAX devices:', jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount Google Drive and clone repository\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!git clone https://github.com/Msingi-AI/msingi1.git\n",
    "%cd msingi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import required libraries\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from transformers import FlaxMoEForCausalLM, AutoTokenizer\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load and prepare dataset\n",
    "def load_dataset():\n",
    "    # Load the pre-trained tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('tokenizer')\n",
    "    \n",
    "    # Load text data\n",
    "    with open('data/Swahili data/Swahili data/train.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Split into chunks of max_length\n",
    "    max_length = 512\n",
    "    stride = 256\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = tokenizer(text, return_tensors='np', truncation=False)['input_ids'][0]\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - max_length + 1, stride):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        if len(chunk) == max_length:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return np.array(chunks), tokenizer\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "chunks, tokenizer = load_dataset()\n",
    "print(f'Created {len(chunks)} training chunks of length {chunks.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define model and training functions\n",
    "def create_train_state(model, learning_rate=1e-4):\n",
    "    \"\"\"Creates training state with Adam optimizer\"\"\"\n",
    "    optimizer = optax.adamw(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.__call__,\n",
    "        params=model.params,\n",
    "        tx=optimizer\n",
    "    )\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch')\n",
    "def train_step(state, batch, rng):\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(\n",
    "            params, batch, \n",
    "            deterministic=False, \n",
    "            rngs={'dropout': rng}\n",
    "        )[0]\n",
    "        \n",
    "        # Shift labels for next-token prediction\n",
    "        labels = jnp.roll(batch, -1, axis=1)\n",
    "        labels = labels.at[:, -1].set(0)  # Mask last token\n",
    "        \n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits[..., :-1, :],\n",
    "            labels[..., 1:]\n",
    "        ).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    \n",
    "    # Average gradients across devices\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    \n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "def create_model(vocab_size):\n",
    "    \"\"\"Create Flax MoE model\"\"\"\n",
    "    config = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'hidden_size': 768,\n",
    "        'num_hidden_layers': 6,\n",
    "        'num_attention_heads': 12,\n",
    "        'intermediate_size': 3072,\n",
    "        'num_experts': 8,\n",
    "        'expert_capacity': 32,\n",
    "        'moe_layer_indices': [2, 4]\n",
    "    }\n",
    "    return FlaxMoEForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Training loop\n",
    "# Initialize wandb\n",
    "wandb.init(project='msingi1', name='tpu_training')\n",
    "\n",
    "# Create model and state\n",
    "model = create_model(tokenizer.vocab_size)\n",
    "state = create_train_state(model)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32 * jax.device_count()  # Batch size per device * num devices\n",
    "num_epochs = 100\n",
    "save_every = 5\n",
    "\n",
    "# Initialize RNG\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rngs = jax.random.split(rng, jax.device_count())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data\n",
    "    rng, shuffle_rng = jax.random.split(rng)\n",
    "    perm = jax.random.permutation(shuffle_rng, len(chunks))\n",
    "    chunks = chunks[perm]\n",
    "    \n",
    "    # Create batches\n",
    "    num_batches = len(chunks) // batch_size\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        batch = chunks[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = jnp.array(batch)\n",
    "        \n",
    "        # Reshape batch for devices\n",
    "        batch = batch.reshape(jax.device_count(), -1, batch.shape[-1])\n",
    "        \n",
    "        # Train step\n",
    "        state, loss = train_step(state, batch, rngs)\n",
    "        epoch_loss += loss.mean()\n",
    "        \n",
    "        # Log to wandb\n",
    "        if i % 10 == 0:\n",
    "            wandb.log({\n",
    "                'loss': loss.mean(),\n",
    "                'epoch': epoch,\n",
    "                'step': epoch * num_batches + i\n",
    "            })\n",
    "    \n",
    "    # Average epoch loss\n",
    "    epoch_loss /= num_batches\n",
    "    print(f'Epoch {epoch+1} loss: {epoch_loss}')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_every == 0:\n",
    "        checkpoint_dir = f'/content/drive/MyDrive/msingi1_checkpoints/epoch_{epoch+1}'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        model.save_pretrained(checkpoint_dir, params=state.params)\n",
    "        print(f'Saved checkpoint to {checkpoint_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate sample text\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors='jax')\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Habari ya leo\"\n",
    "generated = generate_text(prompt)\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Generated: {generated}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "tpu": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
