{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Msingi1: Training on TPU with JAX/Flax\n",
    "\n",
    "This notebook trains the Msingi1 Swahili language model using TPU acceleration and JAX/Flax for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install -q flax transformers datasets wandb\n",
    "!pip install -q git+https://github.com/google/flaxformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check TPU availability\n",
    "import jax\n",
    "print('Number of TPU devices:', jax.device_count())\n",
    "print('JAX devices:', jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Msingi-AI/msingi1.git\n",
    "%cd msingi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from transformers import FlaxPreTrainedModel, PretrainedConfig\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "import wandb\n",
    "\n",
    "# Load our pre-trained tokenizer\n",
    "tokenizer = Tokenizer.from_file('tokenizer/tokenizer.json')\n",
    "\n",
    "# Model configuration\n",
    "class Msingi1Config(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=32000,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=6,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=1024,\n",
    "        num_experts=8,\n",
    "        expert_capacity=32,\n",
    "        moe_layers=[2, 4],\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_capacity = expert_capacity\n",
    "        self.moe_layers = set(moe_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model with Flax MoE\n",
    "from flaxformer.components import dense_attention, dense, mixture_of_experts\n",
    "\n",
    "class ExpertMLP(nn.Module):\n",
    "    config: Msingi1Config\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(\n",
    "            self.config.intermediate_size,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.dense2 = nn.Dense(\n",
    "            self.config.hidden_size,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "\n",
    "    def __call__(self, x, deterministic=True):\n",
    "        x = self.dense1(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x, deterministic=deterministic)\n",
    "        return x\n",
    "\n",
    "class MsingiBlock(nn.Module):\n",
    "    config: Msingi1Config\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "    layer_idx: int = 0\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = dense_attention.MultiHeadDotProductAttention(\n",
    "            num_heads=self.config.num_attention_heads,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        if self.layer_idx in self.config.moe_layers:\n",
    "            self.feed_forward = mixture_of_experts.MoeLayer(\n",
    "                num_experts=self.config.num_experts,\n",
    "                expert_cls=ExpertMLP,\n",
    "                expert_capacity=self.config.expert_capacity,\n",
    "                config=self.config,\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "        else:\n",
    "            self.feed_forward = dense.MlpBlock(\n",
    "                intermediate_dim=self.config.intermediate_size,\n",
    "                dropout_rate=self.config.hidden_dropout_prob,\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm()\n",
    "        self.layernorm2 = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None, deterministic=True):\n",
    "        y = self.layernorm1(x)\n",
    "        y = self.attention(\n",
    "            y, y, mask=mask, deterministic=deterministic\n",
    "        )\n",
    "        x = x + y\n",
    "\n",
    "        y = self.layernorm2(x)\n",
    "        y = self.feed_forward(\n",
    "            y, deterministic=deterministic\n",
    "        )\n",
    "        return x + y\n",
    "\n",
    "class Msingi1Module(nn.Module):\n",
    "    config: Msingi1Config\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    def setup(self):\n",
    "        self.embeddings = nn.Embed(\n",
    "            num_embeddings=self.config.vocab_size,\n",
    "            features=self.config.hidden_size,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.position_embeddings = nn.Embed(\n",
    "            num_embeddings=self.config.max_position_embeddings,\n",
    "            features=self.config.hidden_size,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.layers = [\n",
    "            MsingiBlock(\n",
    "                config=self.config,\n",
    "                layer_idx=i,\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "            for i in range(self.config.num_hidden_layers)\n",
    "        ]\n",
    "        self.layernorm = nn.LayerNorm()\n",
    "        self.lm_head = nn.Dense(\n",
    "            self.config.vocab_size,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask=None, deterministic=True):\n",
    "        seq_length = input_ids.shape[1]\n",
    "        position_ids = jnp.arange(seq_length)[None, :]\n",
    "\n",
    "        x = self.embeddings(input_ids)\n",
    "        x = x + self.position_embeddings(position_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                mask=attention_mask,\n",
    "                deterministic=deterministic\n",
    "            )\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "class FlaxMsingi1PreTrainedModel(FlaxPreTrainedModel):\n",
    "    config_class = Msingi1Config\n",
    "    module_class = Msingi1Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "training_args = {\n",
    "    'per_device_train_batch_size': 32,\n",
    "    'num_train_epochs': 100,\n",
    "    'learning_rate': 3e-4,\n",
    "    'warmup_steps': 1000,\n",
    "    'logging_steps': 100,\n",
    "    'save_steps': 1000,\n",
    "    'output_dir': '/content/drive/MyDrive/msingi1_checkpoints'\n",
    "}\n",
    "\n",
    "# Initialize model and optimizer\n",
    "config = Msingi1Config()\n",
    "model = FlaxMsingi1PreTrainedModel(config)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=training_args['learning_rate'],\n",
    "    b1=0.9,\n",
    "    b2=0.999,\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create training state\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.__call__,\n",
    "    params=model.params,\n",
    "    tx=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training loop with TPU optimization\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(\n",
    "            params,\n",
    "            batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            deterministic=False\n",
    "        )\n",
    "        \n",
    "        # Shift labels for next-token prediction\n",
    "        labels = jnp.roll(batch['input_ids'], -1)\n",
    "        labels = labels.at[:, -1].set(0)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = jnp.mean(loss * batch['attention_mask'])\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='msingi1', name='tpu_training')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(training_args['num_train_epochs']):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        state, loss = train_step(state, batch)\n",
    "        \n",
    "        if step % training_args['logging_steps'] == 0:\n",
    "            wandb.log({\n",
    "                'loss': loss,\n",
    "                'epoch': epoch,\n",
    "                'step': step\n",
    "            })\n",
    "        \n",
    "        if step % training_args['save_steps'] == 0:\n",
    "            model.save_pretrained(\n",
    "                f\"{training_args['output_dir']}/checkpoint-{epoch}-{step}\"\n",
    "            )\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(f\"{training_args['output_dir']}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the model\n",
    "@jax.jit\n",
    "def generate(params, prompt, max_length=100, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt).ids\n",
    "    input_ids = jnp.array(input_ids)[None, :]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        logits = model.apply(\n",
    "            params,\n",
    "            input_ids,\n",
    "            deterministic=True\n",
    "        )\n",
    "        next_token = jax.random.categorical(\n",
    "            jax.random.PRNGKey(0),\n",
    "            logits[:, -1, :] / temperature\n",
    "        )\n",
    "        input_ids = jnp.concatenate([input_ids, next_token[:, None]], axis=1)\n",
    "        \n",
    "        if next_token[0] == tokenizer.token_to_id(\"</s>\"):\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "# Test generation\n",
    "test_prompt = \"Habari ya leo\"\n",
    "generated_text = generate(state.params, test_prompt)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "tpu": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
